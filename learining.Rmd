# Machine Learning Peer Assessments
========================================================
###  using Random Forest Modeling, and got over 99% accuracy on my training set of 60% of the total data. All my submitted test cases were succesful.

## Loading and cleaning Data
```{r,warning=FALSE}
library(ggplot2)
library(caret)
library(caret)

library(e1071)
setwd('C:\\Users\\wenlihaoyu\\Desktop\\PracticalMachine')
testsets <- read.csv("pml-testing.csv",na.strings=c("NA",""))
trainsets <- read.csv("pml-training.csv",na.strings=c("NA",""))
NAs <- apply(trainsets,2,function(x) {sum(is.na(x))}) 
training <- trainsets[,which(NAs == 0)]
testing <- testsets[,which(NAs == 0)]

```

## Building data sets for training and cross validation. 
### Using 60% for training and 40% for Cross Validation.
###  Removing variables that have time, or names in it, also those are the Columns 1..6
```{r,warning=FALSE}
trainIndex <- createDataPartition(y = training$classe, p=0.6,list=FALSE)
trainSet <- training[trainIndex,]
crossValidationSet <- training[-trainIndex,]
removeIndex <- as.integer(c(1,2,3,4,5,6))
trainSet <- training[,-removeIndex]
testSet <- testing[,-removeIndex]
```

## Build random forest model with full training model
```{r,warning=FALSE}
mytrControl = trainControl(method = "cv", number = 4)
modelFit <- train(classe ~.,data = trainSet, method="rf", trControl = mytrControl)
modelFit
```


## Calculate the errors of the Cross Validation Set.
```{r,warning=FALSE}
predicted <- predict(modelFit, crossValidationSet)
SampleError <- sum(predicted == crossValidationSet$classe)/nrow(crossValidationSet)
```
So the Out of Sample Error  is: `r (1-SampleError) *100 ` %


## Get the testset results and Predict testing data
```{r,warning=FALSE}
results <- predict(modelFit, testSet)
results
```
